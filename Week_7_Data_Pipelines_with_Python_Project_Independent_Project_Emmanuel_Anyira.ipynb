{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pipelines with Python Project**\n",
        "\n",
        "**Project Deliverable**\n",
        "\n",
        "‚óè A GitHub repository with a python file (.py) or notebook (.ipynb) with your solution.\n",
        "\n",
        "**Project Description**\n",
        "\n",
        "Telecom companies often have to extract billing data from multiple CSV files generated from various systems and transform it into a structured format for analysis and revenue reporting.\n",
        "This process can be time-consuming, error-prone, and hinder decision-making. Manually analyzing and reconciling billing data from different sources is a tedious task and often leads to delays in generating revenue reports. Thus, there is a need for an automated data pipeline that\n",
        "can extract billing data from multiple sources and transform it into a structured format for efficient analysis and revenue reporting.\n",
        "\n",
        "**Guidelines**\n",
        "Here are some guidelines and hints to help you create the data pipeline:\n",
        "\n",
        "\n",
        "1.   Determine the requirements: First, you need to define the requirements of the data pipeline, including the source and destination of the data, the type of data that needs to be processed, the transformations that need to be applied, and the output format.\n",
        "2.   Extract the data: Use Python to read the CSV files and extract the data.\n",
        "3.   Clean the data: Perform data cleaning on the extracted data to remove any missing values and outliers. For example, you can replace missing values with an appropriate value or remove them altogether.\n",
        "4.   Transform the data: Apply any necessary transformations on the data, such as data type conversion, data aggregation, and data filtering, to prepare the data for analysis.\n",
        "5.   Merge the datasets: Join the different datasets into a single dataset that can be used for analysis.\n",
        "6. Load the data: Load the transformed data into a database or a file, such as a CSV file,that can be easily analyzed.\n",
        "7. Automate the process: Automate the data pipeline by scheduling it to run at a specific time, such as daily or weekly so that it can update the analysis data automatically.\n",
        "8. Test the pipeline: Test the data pipeline to ensure it produces the correct results. This can be done by comparing the results with the expected output or using a test dataset.\n",
        "9. Optimize the pipeline: Optimize the data pipeline to improve performance and reduce errors. This can be done by optimizing the code, parallel processing, and reducing the data size.\n",
        "10. Monitor the pipeline: Monitor the data pipeline to ensure that it runs smoothly and that there are no errors or issues.\n",
        "\n",
        "**Datasets**\n",
        "\n",
        "Here are three sample datasets (https://bit.ly/416WE1X) with billing data that can be joined. The datasets contain some missing values and outliers:\n",
        "\n",
        "1. Dataset 1:\n",
        "* Customer ID (numeric)\n",
        "* Date of purchase (MM/DD/YYYY)\n",
        "* Total amount billed (numeric)\n",
        "* Payment status (categorical - paid, overdue, disputed)\n",
        "* Payment method (categorical - credit card, bank transfer, e-wallet)\n",
        "* Promo code (text)\n",
        "* Country of purchase (categorical)\n",
        "\n",
        "2. Dataset 2:\n",
        "*  Customer ID (numeric)\n",
        "*  Date of payment (MM/DD/YYYY)\n",
        "*  Amount paid (numeric)\n",
        "*  Payment method (categorical - credit card, bank transfer, e-wallet)\n",
        "*  Payment status (categorical - paid, overdue, disputed)\n",
        "*  Late payment fee (numeric)\n",
        "*  Country of payment (categorical)\n",
        "\n",
        "3. Dataset 3:\n",
        "*  Customer ID (numeric)\n",
        "*  Date of refund (MM/DD/YYYY)\n",
        "*  Refund amount (numeric)\n",
        "*  Reason for refund (text)\n",
        "*  Country of refund (categorical)\n",
        "\n",
        "**Notes:**\n",
        "1. The datasets can be joined using Customer ID, Date of purchase/payment/refund, and\n",
        "country of purchase/payment/refund as keys.\n",
        "2. The datasets may contain missing values and outliers for some fields, such as the totaL amount billed or refund amount.\n",
        "3. The payment status may be missing or incomplete for some of the transactions.\n",
        "4. The promo code field may be empty for some of the purchases.\n",
        "5. The reason for the refund may be missing for some of the refund transactions."
      ],
      "metadata": {
        "id": "ppiVNQNFnr1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the required and necessary libraries"
      ],
      "metadata": {
        "id": "ZNHP2ZcJxGiC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lr8JaQXxniIZ"
      },
      "outputs": [],
      "source": [
        "#Importing the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the data from the datasets provided and loading.\n",
        "We use the pandas library to load three datasets into dataframes.\n",
        "\n",
        "The first step defines file paths for the three datasets using relative file paths, assuming that the datasets are in the same directory as the Python script.\n",
        "\n",
        "The next step loads each dataset into a dataframe using pd.read_csv() method of pandas. This allows for quick checks of the datasets, such as identifying missing data, checking naming conventions, and inspecting the general data structure."
      ],
      "metadata": {
        "id": "D7G8aEYKxRj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file paths for the three datasets. We want to see what is inside the datasets\n",
        "file1 = \"dataset1.csv\"\n",
        "file2 = \"dataset2.csv\"\n",
        "file3 = \"dataset3.csv\"\n",
        "\n",
        "# Loading the datasets into dataframes for quick checks(Checking missing data, naming conventions and the general data structure)\n",
        "df1 = pd.read_csv(file1)\n",
        "df2 = pd.read_csv(file2)\n",
        "df3 = pd.read_csv(file3)\n"
      ],
      "metadata": {
        "id": "Q39tBDTgw9gz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can further modify the code to check for any potential errors or issues while loading the data:"
      ],
      "metadata": {
        "id": "Oo1Gew-5Chy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file paths for the three datasets\n",
        "file1 = \"dataset1.csv\"\n",
        "file2 = \"dataset2.csv\"\n",
        "file3 = \"dataset3.csv\"\n",
        "\n",
        "# Load the datasets into dataframes for quick checks\n",
        "try:\n",
        "    df1 = pd.read_csv(file1)\n",
        "    print(f\"Loaded {len(df1)} rows from {file1}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"{file1} not found\")\n",
        "\n",
        "try:\n",
        "    df2 = pd.read_csv(file2)\n",
        "    print(f\"Loaded {len(df2)} rows from {file2}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"{file2} not found\")\n",
        "\n",
        "try:\n",
        "    df3 = pd.read_csv(file3)\n",
        "    print(f\"Loaded {len(df3)} rows from {file3}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"{file3} not found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kfO1taLCoTa",
        "outputId": "cd59e504-7d8e-49a0-c5df-1003b4ba8d39"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 15 rows from dataset1.csv\n",
            "Loaded 15 rows from dataset2.csv\n",
            "Loaded 15 rows from dataset3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking data in df1"
      ],
      "metadata": {
        "id": "9fEun0db6vwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General overview of data in df1\n",
        "print(df1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcuKXVMMw527",
        "outputId": "9d0085c6-4d59-4393-cd8b-2a396219b299"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    customer_id date_of_purchase  total_amount_billed payment_status  \\\n",
            "0           101       04/01/2021                  100           paid   \n",
            "1           102       04/02/2021                  200           paid   \n",
            "2           103       04/02/2021                   50        overdue   \n",
            "3           104       04/03/2021                   75       disputed   \n",
            "4           105       04/04/2021                  125           paid   \n",
            "5           106       04/05/2021                  150           paid   \n",
            "6           107       04/06/2021                   75        overdue   \n",
            "7           108       04/06/2021                  100        overdue   \n",
            "8           109       04/07/2021                   50           paid   \n",
            "9           110       04/07/2021                   25        overdue   \n",
            "10          111       04/08/2021                  175           paid   \n",
            "11          112       04/08/2021                  200           paid   \n",
            "12          113       04/09/2021                   50       disputed   \n",
            "13          114       04/10/2021                  100           paid   \n",
            "14          115       04/10/2021                   75        overdue   \n",
            "\n",
            "   payment_method promo_code country_of_purchase  \n",
            "0     credit card     PROMO1                 USA  \n",
            "1   bank transfer     PROMO2                 USA  \n",
            "2     credit card        NaN                  UK  \n",
            "3        e-wallet     PROMO3                  UK  \n",
            "4     credit card     PROMO4                 USA  \n",
            "5     credit card        NaN                  UK  \n",
            "6        e-wallet     PROMO5                 USA  \n",
            "7   bank transfer     PROMO6                 USA  \n",
            "8   bank transfer        NaN                  UK  \n",
            "9     credit card     PROMO7                 USA  \n",
            "10       e-wallet     PROMO8                  UK  \n",
            "11  bank transfer     PROMO9                 USA  \n",
            "12    credit card    PROMO10                 USA  \n",
            "13    credit card    PROMO11                 USA  \n",
            "14       e-wallet    PROMO12                  UK  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking data in df2"
      ],
      "metadata": {
        "id": "mBD6JFXO6-3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General overview of data in df2\n",
        "print(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WdpmdyDs14E",
        "outputId": "e8800042-9ead-43b9-82ef-c91512bed92b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    customer_id date_of_payment  amount_paid payment_method payment_status  \\\n",
            "0           101      04/01/2021          100    credit card           paid   \n",
            "1           102      04/03/2021          200  bank transfer           paid   \n",
            "2           103      04/03/2021           75    credit card           paid   \n",
            "3           104      04/04/2021           50       e-wallet        overdue   \n",
            "4           105      04/05/2021          125    credit card           paid   \n",
            "5           106      04/06/2021          150    credit card           paid   \n",
            "6           107      04/07/2021           75       e-wallet        overdue   \n",
            "7           108      04/07/2021          100  bank transfer        overdue   \n",
            "8           109      04/08/2021           50  bank transfer           paid   \n",
            "9           110      04/08/2021           25    credit card           paid   \n",
            "10          111      04/09/2021          175       e-wallet           paid   \n",
            "11          112      04/10/2021          200  bank transfer           paid   \n",
            "12          113      04/10/2021           50    credit card       disputed   \n",
            "13          114      04/11/2021          100    credit card           paid   \n",
            "14          115      04/12/2021           75       e-wallet        overdue   \n",
            "\n",
            "    late_payment_fee country_of_payment  \n",
            "0                  0                USA  \n",
            "1                  0                USA  \n",
            "2                 10                 UK  \n",
            "3                  0                 UK  \n",
            "4                  0                USA  \n",
            "5                  0                 UK  \n",
            "6                 20                USA  \n",
            "7                 30                USA  \n",
            "8                  0                 UK  \n",
            "9                  0                USA  \n",
            "10                 0                 UK  \n",
            "11                 0                USA  \n",
            "12                 0                USA  \n",
            "13                 0                USA  \n",
            "14                15                 UK  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking data in df3"
      ],
      "metadata": {
        "id": "-6ltHgty7Dkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General overview of data in df3\n",
        "print(df3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHBOU-3Vs41q",
        "outputId": "2ccacd23-ae2c-4a5b-9d68-e9081bfe6b48"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    customer_id date_of_refund  refund_amount         reason_for_refund  \\\n",
            "0           101     04/03/2021            100  product not as described   \n",
            "1           102     04/06/2021            200         defective product   \n",
            "2           103     04/07/2021             75            change of mind   \n",
            "3           104     04/08/2021             50      product not received   \n",
            "4           105     04/09/2021             25  product not as described   \n",
            "5           106     04/11/2021            125         defective product   \n",
            "6           107     04/12/2021            150            change of mind   \n",
            "7           108     04/13/2021             75  product not as described   \n",
            "8           109     04/13/2021            100         defective product   \n",
            "9           110     04/14/2021             50      product not received   \n",
            "10          111     04/15/2021            175         defective product   \n",
            "11          112     04/16/2021            200            change of mind   \n",
            "12          113     04/16/2021             50  product not as described   \n",
            "13          114     04/17/2021            100         defective product   \n",
            "14          115     04/18/2021             75      product not received   \n",
            "\n",
            "   country_of_refund  \n",
            "0                USA  \n",
            "1                USA  \n",
            "2                 UK  \n",
            "3                 UK  \n",
            "4                USA  \n",
            "5                 UK  \n",
            "6                USA  \n",
            "7                USA  \n",
            "8                 UK  \n",
            "9                USA  \n",
            "10                UK  \n",
            "11               USA  \n",
            "12               USA  \n",
            "13               USA  \n",
            "14                UK  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the paths to the datasets afresh(Input and Output paths)\n",
        "\n",
        "```\n",
        "# Let us define new paths for more analysis\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "70f4y0nAx1f1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the file paths for three input datasets (set1_path, set2_path, and set3_path) and an output file path (output_path).\n",
        "\n",
        "The relative paths specified assume that the datasets and the output file will be stored in the same directory as the Python script."
      ],
      "metadata": {
        "id": "yTWlrLdXDZ0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining new paths to the CSV files\n",
        "set1_path = \"dataset1.csv\"\n",
        "set2_path = \"dataset2.csv\"\n",
        "set3_path = \"dataset3.csv\"\n",
        "\n",
        "# Defining the output path for the transformed data. We will call the transformed CSV \"output.csv\"\n",
        "output_path = \"final_output.csv\""
      ],
      "metadata": {
        "id": "vOeyLv3Yx1Nl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can modify the code to include error handling for the file paths defined above:"
      ],
      "metadata": {
        "id": "INJ_cmefDfsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set1_path = \"dataset1.csv\"\n",
        "set2_path = \"dataset2.csv\"\n",
        "set3_path = \"dataset3.csv\"\n",
        "output_path = \"final_output.csv\"\n",
        "\n",
        "# Check if input file paths exist\n",
        "if not all(map(os.path.isfile, [set1_path, set2_path, set3_path])):\n",
        "    raise FileNotFoundError(\"One or more input files not found\")\n",
        "\n",
        "# Check if output file path exists\n",
        "if os.path.isfile(output_path):\n",
        "    print(\"Output file already exists. It will be overwritten.\")\n",
        "\n",
        "# Use absolute paths instead of relative paths\n",
        "set1_path = os.path.abspath(set1_path)\n",
        "set2_path = os.path.abspath(set2_path)\n",
        "set3_path = os.path.abspath(set3_path)\n",
        "output_path = os.path.abspath(output_path)\n"
      ],
      "metadata": {
        "id": "N5trUVM1Dnj1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming the data: \n",
        "\n",
        "\n",
        "```\n",
        "# We need to apply any necessary transformations on the data, such as data type conversion, filling missing values, ensuring everything is in lowercase and replace spaces in variable names with underscores\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "YZUS3qQZyXo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define the data types for each column in the three input datasets using dictionaries.\n",
        "\n",
        "For example, set1_dtypes specifies the data types for the columns in the dataset1.csv file. The customer_id column should be of type np.int64, the total_amount_billed column should be of type np.float64, the payment_status and payment_method columns should be of type \"category\", the promo_code column should be of type str, and the country_of_purchase column should be of type \"category\". The commented out lines indicate that the original code anticipated having columns representing dates, but those are not being used.\n",
        "\n",
        "Using the correct data types is important because it can save memory, reduce the possibility of errors in data analysis and visualization, and improve performance."
      ],
      "metadata": {
        "id": "XTPcPsnqEpXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the data types for each column in the datasets read using the paths\n",
        "set1_dtypes = {\n",
        "    \"customer_id\": np.int64,\n",
        "    # \"date_of_purchase\": np.datetime64,\n",
        "    \"total_amount_billed\": np.float64,\n",
        "    \"payment_status\": \"category\",\n",
        "    \"payment_method\": \"category\",\n",
        "    \"promo_code\": str,\n",
        "    \"country_of_purchase\": \"category\"\n",
        "}\n",
        "\n",
        "set2_dtypes = {\n",
        "    \"customer_id\": np.int64,\n",
        "    # \"date_of_payment\": np.datetime64,\n",
        "    \"amount_paid\": np.float64,\n",
        "    \"payment_method\": \"category\",\n",
        "    \"payment_status\": \"category\",\n",
        "    \"late_payment_fee\": np.float64,\n",
        "    \"country_of_payment\": \"category\"\n",
        "}\n",
        "\n",
        "set3_dtypes = {\n",
        "    \"customer_id\": np.int64,\n",
        "    # \"date_of_refund\": np.datetime64,\n",
        "    \"refund_amount\": np.float64,\n",
        "    \"reason_for_refund\": str,\n",
        "    \"country_of_refund\": \"category\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "4EiV51hJyXUT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We the define three lambda functions to parse dates in each of the three datasets.\n",
        "\n",
        "The set1_date_parser function takes a date string in the format \"month/day/year\" (e.g., \"01/25/2022\") and converts it to a Pandas datetime object. The format argument specifies the format of the input date string, and the errors argument tells Pandas to set any invalid date strings to NaT (Not a Time) instead of raising an error.\n",
        "\n",
        "The set2_date_parser and set3_date_parser functions are identical to set1_date_parser, but they are defined for the second and third datasets, respectively.\n",
        "\n",
        "Using the correct date parsers is important because it ensures that date columns are correctly interpreted and can be used in time-based analyses, such as time series forecasting or trend analysis."
      ],
      "metadata": {
        "id": "WiFx3CzHFJ00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the date parsers for each dataset\n",
        "set1_date_parser = lambda x: pd.to_datetime(x, format=\"%m/%d/%Y\", errors=\"coerce\")\n",
        "set2_date_parser = lambda x: pd.to_datetime(x, format=\"%m/%d/%Y\", errors=\"coerce\")\n",
        "set3_date_parser = lambda x: pd.to_datetime(x, format=\"%m/%d/%Y\", errors=\"coerce\")"
      ],
      "metadata": {
        "id": "RmSH9kUg7kj0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then read CSV files using pandas and specify data types and date parsers for each column. Additionally, we are trying to parse the dates while reading the CSV files.\n"
      ],
      "metadata": {
        "id": "gitvw-DAFSRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the datasets into pandas dataframes\n",
        "set1 = pd.read_csv(set1_path, dtype=set1_dtypes, parse_dates=[\"date_of_purchase\"], date_parser=set1_date_parser)\n",
        "set2 = pd.read_csv(set2_path, dtype=set2_dtypes, parse_dates=[\"date_of_payment\"], date_parser=set2_date_parser)\n",
        "set3 = pd.read_csv(set3_path, dtype=set3_dtypes, parse_dates=[\"date_of_refund\"], date_parser=set3_date_parser)"
      ],
      "metadata": {
        "id": "MAs-_YSxylsK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the data: We will use all the methods to ensure data is clean"
      ],
      "metadata": {
        "id": "ocEtmT6WyspU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We replace missing values, but note that replacing all missing values with 0 can distort your data and produce incorrect analysis results, especially for numerical data. You may want to consider other approaches for handling missing data such as imputation or removal of rows/columns with missing values depending on your specific use case."
      ],
      "metadata": {
        "id": "9hHQ1EJiF2Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values in Total amount billed and Refund amount with 0\n",
        "set1.replace([\"\", \" \", \"-\"], np.nan, inplace=True)\n",
        "set2.replace([\"\", \" \", \"-\"], np.nan, inplace=True)\n",
        "set3.replace([\"\", \" \", \"-\"], np.nan, inplace=True)\n"
      ],
      "metadata": {
        "id": "rL6HADCLsN2s"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging the datasets:"
      ],
      "metadata": {
        "id": "JNrWhz7Jy4JC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first print the columns for the three respective datasets. Printing the columns of each dataset individually, help us identify which columns can be merged."
      ],
      "metadata": {
        "id": "CkhRm-jM8i7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the data for merging by checking their column names to determine which columns can be merged\n",
        "print(f'Dataset 1\\n{set1.columns}')\n",
        "print(f'Dataset 2\\n{set2.columns}')\n",
        "print(f'Dataset 3\\n{set3.columns}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwGRA9wGtC2L",
        "outputId": "61caae53-2336-4093-fe75-38cc59a9ca7b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 1\n",
            "Index(['customer_id', 'date', 'total_amount_billed', 'payment_status',\n",
            "       'payment_method', 'promo_code', 'country'],\n",
            "      dtype='object')\n",
            "Dataset 2\n",
            "Index(['customer_id', 'date', 'amount_paid', 'payment_method',\n",
            "       'payment_status', 'late_payment_fee', 'country'],\n",
            "      dtype='object')\n",
            "Dataset 3\n",
            "Index(['customer_id', 'date', 'refund_amount', 'reason_for_refund', 'country'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then reset the index of each dataset and drops the old index column. This is done to avoid duplicate index values when we merge the datasets."
      ],
      "metadata": {
        "id": "JGWXR_wi9QE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping the index columns\n",
        "set1 = set1.reset_index(drop=True)\n",
        "set2 = set2.reset_index(drop=True)\n",
        "set3 = set3.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "XT8qBkUMzDOM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then rename the columns to a common naming convention, which is a good practice for consistency in data."
      ],
      "metadata": {
        "id": "KNv5CEuP9VPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the columns\n",
        "set1 = set1.rename(columns={\"date_of_purchase\": \"date\", \"country_of_purchase\": \"country\"})\n",
        "set2 = set2.rename(columns={\"date_of_payment\": \"date\", \"country_of_payment\": \"country\"})\n",
        "set3 = set3.rename(columns={\"date_of_refund\": \"date\", \"country_of_refund\": \"country\"})"
      ],
      "metadata": {
        "id": "tHrpigBB87wr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We the successfully merge the three datasets using Customer ID, Date of purchase/payment/refund, and country of purchase/payment/refund as keys. The resulting merged_data dataframe now contains the columns from all three datasets.\n",
        "\n",
        "However, before proceeding further, it is important to check the merged data for missing values and outliers as mentioned in the project description. You may need to perform some data cleaning and transformation steps to prepare the data for analysis."
      ],
      "metadata": {
        "id": "2VDmJYb49jYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the datasets by Customer ID, Date of purchase/payment/refund, and country of purchase/payment/refund\n",
        "merged_data = pd.merge(set1, set2, on=['customer_id','date','country'],\n",
        "                          how='outer', suffixes = ('_purchase', '_payment'))\n",
        "\n",
        "merged_data = pd.merge(merged_data, set3, on=['customer_id','date','country'], \n",
        "                          how='outer', suffixes = ('_payment', '_refund'))\n",
        "merged_data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "ln4_O34ptmKT",
        "outputId": "26005748-c010-49a2-f8b6-4ed4d519b97a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   customer_id       date  total_amount_billed payment_status_purchase  \\\n",
              "0          101 2021-04-01                100.0                    paid   \n",
              "1          102 2021-04-02                200.0                    paid   \n",
              "2          103 2021-04-02                 50.0                 overdue   \n",
              "3          104 2021-04-03                 75.0                disputed   \n",
              "4          105 2021-04-04                125.0                    paid   \n",
              "\n",
              "  payment_method_purchase promo_code country  amount_paid  \\\n",
              "0             credit card     PROMO1     USA        100.0   \n",
              "1           bank transfer     PROMO2     USA          NaN   \n",
              "2             credit card        NaN      UK          NaN   \n",
              "3                e-wallet     PROMO3      UK          NaN   \n",
              "4             credit card     PROMO4     USA          NaN   \n",
              "\n",
              "  payment_method_payment payment_status_payment  late_payment_fee  \\\n",
              "0            credit card                   paid               0.0   \n",
              "1                    NaN                    NaN               NaN   \n",
              "2                    NaN                    NaN               NaN   \n",
              "3                    NaN                    NaN               NaN   \n",
              "4                    NaN                    NaN               NaN   \n",
              "\n",
              "   refund_amount reason_for_refund  \n",
              "0            NaN               NaN  \n",
              "1            NaN               NaN  \n",
              "2            NaN               NaN  \n",
              "3            NaN               NaN  \n",
              "4            NaN               NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-888d2dd3-56c4-47ff-9c08-d027297177bd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_id</th>\n",
              "      <th>date</th>\n",
              "      <th>total_amount_billed</th>\n",
              "      <th>payment_status_purchase</th>\n",
              "      <th>payment_method_purchase</th>\n",
              "      <th>promo_code</th>\n",
              "      <th>country</th>\n",
              "      <th>amount_paid</th>\n",
              "      <th>payment_method_payment</th>\n",
              "      <th>payment_status_payment</th>\n",
              "      <th>late_payment_fee</th>\n",
              "      <th>refund_amount</th>\n",
              "      <th>reason_for_refund</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>101</td>\n",
              "      <td>2021-04-01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>paid</td>\n",
              "      <td>credit card</td>\n",
              "      <td>PROMO1</td>\n",
              "      <td>USA</td>\n",
              "      <td>100.0</td>\n",
              "      <td>credit card</td>\n",
              "      <td>paid</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>102</td>\n",
              "      <td>2021-04-02</td>\n",
              "      <td>200.0</td>\n",
              "      <td>paid</td>\n",
              "      <td>bank transfer</td>\n",
              "      <td>PROMO2</td>\n",
              "      <td>USA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>103</td>\n",
              "      <td>2021-04-02</td>\n",
              "      <td>50.0</td>\n",
              "      <td>overdue</td>\n",
              "      <td>credit card</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>104</td>\n",
              "      <td>2021-04-03</td>\n",
              "      <td>75.0</td>\n",
              "      <td>disputed</td>\n",
              "      <td>e-wallet</td>\n",
              "      <td>PROMO3</td>\n",
              "      <td>UK</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>105</td>\n",
              "      <td>2021-04-04</td>\n",
              "      <td>125.0</td>\n",
              "      <td>paid</td>\n",
              "      <td>credit card</td>\n",
              "      <td>PROMO4</td>\n",
              "      <td>USA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-888d2dd3-56c4-47ff-9c08-d027297177bd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-888d2dd3-56c4-47ff-9c08-d027297177bd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-888d2dd3-56c4-47ff-9c08-d027297177bd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just in case it bring issues renaming the columns, you can merge using the original names"
      ],
      "metadata": {
        "id": "CEXiDvNi-xvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the columns of each dataset\n",
        "print(f\"Columns in set1: {set1.columns}\")\n",
        "print(f\"Columns in set2: {set2.columns}\")\n",
        "print(f\"Columns in set3: {set3.columns}\")\n",
        "\n",
        "# Merge the datasets by Customer ID, Date of purchase/payment/refund, and country of purchase/payment/refund\n",
        "#merged_data = pd.merge(set1, set2, on=['customer_id','date_of_purchase','country_of_purchase'],\n",
        "#                          how='outer', suffixes = ('_purchase', '_payment'))\n",
        "\n",
        "#merged_data = pd.merge(merged_data, set3, on=['customer_id','date_of_purchase','country_of_purchase'], \n",
        "#                          how='outer', suffixes = ('_payment', '_refund'))\n",
        "#merged_data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgpOtnTS9_dr",
        "outputId": "2906ff00-08db-43c3-985e-30f6ca9865e4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in set1: Index(['customer_id', 'date', 'total_amount_billed', 'payment_status',\n",
            "       'payment_method', 'promo_code', 'country'],\n",
            "      dtype='object')\n",
            "Columns in set2: Index(['customer_id', 'date', 'amount_paid', 'payment_method',\n",
            "       'payment_status', 'late_payment_fee', 'country'],\n",
            "      dtype='object')\n",
            "Columns in set3: Index(['customer_id', 'date', 'refund_amount', 'reason_for_refund', 'country'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further and deeper cleaing of the data for finer and refined analysis"
      ],
      "metadata": {
        "id": "oYfD2pw6zOli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the data further to prepair it for deeper analysis \n",
        "\n",
        "# Replacing the missing values in 'amount_paid' with 0\n",
        "merged_data['amount_paid'].fillna(value=0, inplace=True)\n",
        "\n",
        "# Replace missing values in 'late_payment_fee' with 0\n",
        "merged_data['late_payment_fee'].fillna(value=0, inplace=True)\n",
        "\n",
        "\n",
        "# Replace missing values in 'refund_amount' with 0\n",
        "merged_data['refund_amount'].fillna(value=0, inplace=True)"
      ],
      "metadata": {
        "id": "XT0Ns8HlvlDj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the revenue by subtracting the refund amount and late payment fee from the total amount billed\n",
        "merged_data[\"revenue\"] = merged_data[\"total_amount_billed\"] - merged_data[\"refund_amount\"] - merged_data[\"late_payment_fee\"]"
      ],
      "metadata": {
        "id": "OwjXTOs3tvjT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data to an external file: After successfully merging the datasets, we then save the transformed data into a CSV file. \n",
        "Printing the first 5 rows of the merged dataset confirms that the merge of the three data sets was successful."
      ],
      "metadata": {
        "id": "j0zX-Jf_zcA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the transformed data into a CSV file\n",
        "merged_data.to_csv(output_path, index=False)\n",
        "\n",
        "# Print the first 5 rows of the transformed data\n",
        "merged_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "BolwlV9JzbEc",
        "outputId": "f84c7723-ef90-404a-b4a3-d1b1796b67a5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   customer_id       date  total_amount_billed payment_status_purchase  \\\n",
              "0          101 2021-04-01                100.0                    paid   \n",
              "1          102 2021-04-02                200.0                    paid   \n",
              "2          103 2021-04-02                 50.0                 overdue   \n",
              "3          104 2021-04-03                 75.0                disputed   \n",
              "4          105 2021-04-04                125.0                    paid   \n",
              "\n",
              "  payment_method_purchase promo_code country  amount_paid  \\\n",
              "0             credit card     PROMO1     USA        100.0   \n",
              "1           bank transfer     PROMO2     USA          0.0   \n",
              "2             credit card        NaN      UK          0.0   \n",
              "3                e-wallet     PROMO3      UK          0.0   \n",
              "4             credit card     PROMO4     USA          0.0   \n",
              "\n",
              "  payment_method_payment payment_status_payment  late_payment_fee  \\\n",
              "0            credit card                   paid               0.0   \n",
              "1                    NaN                    NaN               0.0   \n",
              "2                    NaN                    NaN               0.0   \n",
              "3                    NaN                    NaN               0.0   \n",
              "4                    NaN                    NaN               0.0   \n",
              "\n",
              "   refund_amount reason_for_refund  revenue  \n",
              "0            0.0               NaN    100.0  \n",
              "1            0.0               NaN    200.0  \n",
              "2            0.0               NaN     50.0  \n",
              "3            0.0               NaN     75.0  \n",
              "4            0.0               NaN    125.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c988cfb-f997-4c31-a16d-74285e4ab5b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_id</th>\n",
              "      <th>date</th>\n",
              "      <th>total_amount_billed</th>\n",
              "      <th>payment_status_purchase</th>\n",
              "      <th>payment_method_purchase</th>\n",
              "      <th>promo_code</th>\n",
              "      <th>country</th>\n",
              "      <th>amount_paid</th>\n",
              "      <th>payment_method_payment</th>\n",
              "      <th>payment_status_payment</th>\n",
              "      <th>late_payment_fee</th>\n",
              "      <th>refund_amount</th>\n",
              "      <th>reason_for_refund</th>\n",
              "      <th>revenue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>101</td>\n",
              "      <td>2021-04-01</td>\n",
              "      <td>100.0</td>\n",
              "      <td>paid</td>\n",
              "      <td>credit card</td>\n",
              "      <td>PROMO1</td>\n",
              "      <td>USA</td>\n",
              "      <td>100.0</td>\n",
              "      <td>credit card</td>\n",
              "      <td>paid</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>102</td>\n",
              "      <td>2021-04-02</td>\n",
              "      <td>200.0</td>\n",
              "      <td>paid</td>\n",
              "      <td>bank transfer</td>\n",
              "      <td>PROMO2</td>\n",
              "      <td>USA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>103</td>\n",
              "      <td>2021-04-02</td>\n",
              "      <td>50.0</td>\n",
              "      <td>overdue</td>\n",
              "      <td>credit card</td>\n",
              "      <td>NaN</td>\n",
              "      <td>UK</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>104</td>\n",
              "      <td>2021-04-03</td>\n",
              "      <td>75.0</td>\n",
              "      <td>disputed</td>\n",
              "      <td>e-wallet</td>\n",
              "      <td>PROMO3</td>\n",
              "      <td>UK</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>105</td>\n",
              "      <td>2021-04-04</td>\n",
              "      <td>125.0</td>\n",
              "      <td>paid</td>\n",
              "      <td>credit card</td>\n",
              "      <td>PROMO4</td>\n",
              "      <td>USA</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>125.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c988cfb-f997-4c31-a16d-74285e4ab5b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c988cfb-f997-4c31-a16d-74285e4ab5b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c988cfb-f997-4c31-a16d-74285e4ab5b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Automating  the process using cron job:"
      ],
      "metadata": {
        "id": "Q0e8YFWPzjji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can auotmate the process by setting up a cron job to execute automatically as desired \n",
        "# Here's an example of how to set up a cron job to run the data pipeline script every day at 3:00 AM:\n",
        "\n",
        "0 3 * * * /path/to/python /path/to/data_pipeline.py"
      ],
      "metadata": {
        "id": "mr07KUHpznOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This unix crontab ebtry, schedules a command to run at a specified time interval.\n",
        "\n",
        "The entry \"0 3 * * *\" means that the command will be executed at 3:00 AM every day.\n",
        "\n",
        "The command itself is \"/path/to/python /path/to/python_data_pipeline.py\", which is a Python script that likely performs some data processing or analysis.\n",
        "\n",
        "Overall, this crontab entry schedules a daily data processing job to run at 3:00 AM using the specified Python script."
      ],
      "metadata": {
        "id": "Mu5S_vtUAF1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use bash script and cron job simultaneously to schedule this job. \n",
        "Here's an example bash script you can use for the cron job:"
      ],
      "metadata": {
        "id": "YLm6kcfoAyss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "python /path/to/python_data_pipeline.py\n"
      ],
      "metadata": {
        "id": "9CLBxjZ7BAPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the script with a filename such as run_python_data_pipeline.sh in the same directory as your Python script.\n",
        "\n",
        "Make sure to give the script execute permissions using the command chmod +x run_python_data_pipeline.sh.\n",
        "\n",
        "Then, update your cron job to execute the bash script at the desired time:"
      ],
      "metadata": {
        "id": "eIOgEdWbBDRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "0 3 * * * /path/to/run_data_pipeline.sh\n"
      ],
      "metadata": {
        "id": "zhThMXCpBTWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will execute the Python script at 3:00 AM every day using the run_python_data_pipeline.sh script."
      ],
      "metadata": {
        "id": "Ee94WFvNBRhr"
      }
    }
  ]
}